{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 收集数据部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## collect_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[查看代码](https://github.com/ricagj/pysekiro_with_RL/blob/main/pysekiro/collect_data.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 get_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原本的收集数据的策略是采取独热编码的，现在变成一个前半部分由独热编码（攻防垫跳和其他）组成和后半部分由道具使用信息（伤药葫芦、BUFF）和位移信息（前后左右）组成的列表  \n",
    "目的是期望这些新增的信息在将来能够缓解数据不平衡，不过当前参加训练的依然是前面独热编码部分（通过列表切片的方式取出独热编码部分），新增的部分暂时没有用到"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~python\n",
    "def get_output(keys):\n",
    "\n",
    "    output = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "    # 对参与训练的按键信息进行独热编码\n",
    "    # 攻击、防御、垫步、跳跃和使用道具不能同时进行（指0.1秒内），但是可以和移动同时进行\n",
    "    if   'J' in keys:\n",
    "        output[0] = 1    # 等同于[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    elif 'K' in keys:\n",
    "        output[1] = 1    # 等同于[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    elif 'LSHIFT' in keys:\n",
    "        output[2] = 1    # 等同于[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "    elif 'SPACE' in keys:\n",
    "        output[3] = 1    # 等同于[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "    elif 'R' in keys:\n",
    "        output[5] = 1    # 等同于[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]    不参与训练\n",
    "        output[4] = 1    # 等同于[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "    else:\n",
    "        output[4] = 1    # 等同于[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "\n",
    "    # 不能同时前后移动\n",
    "    if   'W' in keys:\n",
    "        output[6] = 1    # 等同于[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]    不参与训练\n",
    "    elif 'S' in keys:\n",
    "        output[7] = 1    # 等同于[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]    不参与训练\n",
    "\n",
    "    # 不能同时左右移动\n",
    "    if   'A' in keys:\n",
    "        output[8] = 1    # 等同于[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]    不参与训练\n",
    "    elif 'D' in keys:\n",
    "        output[9] = 1    # 等同于[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]    不参与训练\n",
    "\n",
    "    return output\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 class Data_collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~python\n",
    "def __init__(self, target):\n",
    "    self.target = target    # 目标\n",
    "    self.dataset = list()    # 保存数据的容器\n",
    "    self.save_path = os.path.join('The_battle_memory', self.target)    # 保存的位置\n",
    "    if not os.path.exists(self.save_path):    # 确保保存的位置存在\n",
    "        os.mkdir(self.save_path)\n",
    "\n",
    "    self.step = 0    # 计步器\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 保存数据方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保存数据，统一名称为 '**training_data-** 序号 **.npy**'，例如 training_data-123.npy  \n",
    "从序号为1开始检测文件名是否存在于保存路径，直到检测到不存在就保存，主要是为了防止文件名重复不小心把原本的文件覆盖了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~python\n",
    "def save_data(self):\n",
    "    n = 1\n",
    "    while True:    # 直到找到保存位置并保存就 break\n",
    "        filename = f'training_data-{n}.npy'\n",
    "        save_path = os.path.join(self.save_path, filename)\n",
    "        if not os.path.exists(save_path):    # 没有重复的文件名就执行保存并退出\n",
    "            print(save_path)\n",
    "            np.save(save_path, self.dataset)\n",
    "            break\n",
    "        n += 1\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 收集数据方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这部分代码的作用是：记录当前这个循环开始的时间，不断检测按键信息（核心部分也要用到这个按键信息），按 'T' 取消暂停状态开始收集数据，最后按 'P' 退出循环，保存数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~python\n",
    "def collect_data(self):\n",
    "\n",
    "    paused = True\n",
    "    while True:\n",
    "        last_time = time.time()\n",
    "        keys = key_check()\n",
    "        if paused:\n",
    "            if 'T' in keys:\n",
    "                paused = False\n",
    "        else:\n",
    "\n",
    "            self.step += 1\n",
    "\n",
    "            \"\"\"\n",
    "            核心部分，看下面\n",
    "            \"\"\"\n",
    "\n",
    "            if 'P' in keys:    # 结束\n",
    "                self.save_data()    # 保存数据\n",
    "                break\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**核心部分**  \n",
    "\n",
    "~~~python\n",
    "screen = get_screen()    # 获取屏幕图像\n",
    "if not (np.sum(screen == 0) > 97200):    # 270 * 480 * 3 / 4 = 97200 ，当图像有1/4变成黑色（像素值为0）的时候停止暂停收集数据\n",
    "    action_list = get_output(keys)    # 获取按键输出列表\n",
    "    self.dataset.append([screen, action_list])    # 图像和输出打包在一起，保证一一对应\n",
    "\n",
    "# 降低数据采集的频率，周期为0.1秒\n",
    "T = 0.1\n",
    "t = T-(time.time()-last_time)\n",
    "if t > 0:\n",
    "    time.sleep(t)\n",
    "~~~\n",
    "\n",
    "- 记录数据的条件是非黑屏  \n",
    "- 数据采集降频的目的主要是提高 action 与 reward 的对应程度，其次是缓解数据不平衡和\n",
    "    - reward 是执行当前动作后观测状态变化所得到的反馈，如果采集频率过高，会导致状态还没变化就计算了 reward ，这样就无法得到正确的 reward 。action 与 reward 不对应会导致训练出现严重的后果甚至训练结果是无效的。\n",
    "        - 注：当前设置的采集频率 f=10 只是当前的标准，如果未来发现更合适的采集频率，可能会更改此参数\n",
    "    - 数据采集降频理论上不能改变各类动作的数据占比，但能够缩小了数据量的差距\n",
    "        - 按键检测原理详见[了解项目基础部分是如何工作的](https://github.com/ricagj/pysekiro_with_RL/blob/main/How_it_works.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 数据预览"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以先看这个演示 https://github.com/ricagj/pysekiro/blob/main/imgs/data_preview.gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注：以下展示的是我本地的数据集，我没有上传，所以要运行以下代码，请先自己收集至少一个数据集**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data_quality (存放数据集对应的reward曲线)\n",
    "    - Genichiro_Ashina （苇名弦一郎）\n",
    "        - training_data-1.png （第一个战斗数据的reward曲线）\n",
    "-\n",
    "- The_battle_memory\n",
    "    - Genichiro_Ashina （苇名弦一郎）\n",
    "        - training_data-1.npy （第一个战斗数据）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "target = 'Genichiro_Ashina' # 苇名弦一郎\n",
    "# target = 'Inner_Genichiro' # 心中的弦一郎\n",
    "# target = 'Isshin,_the_Sword_Saint' # 剑圣一心\n",
    "# target = 'Inner_Isshin' # 心中的一心\n",
    "\n",
    "path = os.path.join('The_battle_memory', target, f'training_data-{1}.npy')\n",
    "if os.path.exists(path):\n",
    "    data = np.load(path, allow_pickle=True)\n",
    "    print(f'当前数据集所包含数据量：{data.shape[0]}, 每个数据都有 {data.shape[1]} 个数据，分别是图像数据和按键数据')\n",
    "else:\n",
    "    print('数据集不存在，以下代码无法运行')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "screen = data[0][0]    # 选取了第一个数据的图像部分\n",
    "# print('图像', screen)\n",
    "print('形状', screen.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_value = data[0][1]    # 选取了第一个数据的按键记录部分\n",
    "print('按键记录', action_value)\n",
    "print('目前的训练只使用前五个，即独热编码部分  ', action_value[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计动作分布\n",
    "def motionCounts(Y):\n",
    "\n",
    "    columns=['攻击', '弹反', '垫步', '跳跃', '无键', '道具', '向前', '向后', '向左', '向右']\n",
    "    df = pd.DataFrame(Y, columns=columns, dtype=np.uint8)\n",
    "    \n",
    "    total = len(df)\n",
    "\n",
    "    for motion in columns:\n",
    "        motion_count = len(df)-df[motion].value_counts()[0]\n",
    "        print(f'motion: {motion:>3}, 记录到的次数: {motion_count:>5}')\n",
    "\n",
    "Y = np.array([i[1] for i in data])\n",
    "motionCounts(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysekiro.get_vertices import roi\n",
    "\n",
    "print()\n",
    "print('以视频的形式展示数据。', '要你自己运行才看得见视频')\n",
    "Remaining = len(data)\n",
    "for screen, action_value in data:\n",
    "    action = np.argmax(action_value[:5])\n",
    "\n",
    "    if   action == 0:\n",
    "        a = '攻击'\n",
    "    elif action == 1:\n",
    "        a = '弹反'\n",
    "    elif action == 2:\n",
    "        a = '垫步'\n",
    "    elif action == 3:\n",
    "        a = '跳跃'\n",
    "    elif action == 4:\n",
    "        a = '其他'\n",
    "\n",
    "    cv2.imshow('screen', screen)\n",
    "    cv2.imshow('roi', roi(screen, x=140, x_w=340, y=30, y_h=230))\n",
    "    cv2.waitKey(10)\n",
    "\n",
    "    Remaining -= 1\n",
    "    print(f'\\r 剩余: {Remaining:>4}, 动作:{a} {action_value}', end='') # end='\\n'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):    # 按 q 键关闭视频\n",
    "        cv2.destroyAllWindows()\n",
    "        break\n",
    "else:\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取数据集对应的reward曲线\n",
    "from pysekiro.Agent import get_data_quality\n",
    "\n",
    "get_data_quality()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## learn_offline.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[查看代码](https://github.com/ricagj/pysekiro_with_RL/blob/main/learn_offline.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~python\n",
    "# 依次读取训练集进行离线学习\n",
    "for i in range(start, end+1):\n",
    "\n",
    "    filename = f'training_data-{i}.npy'\n",
    "    data_path = os.path.join('The_battle_memory', target, filename)\n",
    "\n",
    "    if os.path.exists(data_path):    # 确保数据集存在\n",
    "\n",
    "        # 加载数据集\n",
    "        data = np.load(data_path, allow_pickle=True)\n",
    "\n",
    "        \"\"\"\n",
    "        学习部分，看下面\n",
    "        \"\"\"\n",
    "        \n",
    "        sekiro_agent.save_evaluate_network()    # 这个数据学习完毕，保存网络权重\n",
    "        sekiro_agent.reward_system.save_reward_curve(save_path=reward_curve_save_path)    # 绘制 reward 曲线并保存\n",
    "\n",
    "    else:\n",
    "        print(f'{filename} does not exist ')\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**学习部分**  \n",
    "主要思路：获取 (状态S、动作A、奖励R 和 新状态S')，存储以供经验回放，然后目标网络和评估网络根据各自设置的更新频率更新自身网络参数  \n",
    "\n",
    "~~~python\n",
    "sekiro_agent.reward_system.cur_status = get_status(dataset[0][0])    # 设置初始状态\n",
    "for step in range(len(data)-1):\n",
    "\n",
    "    # ---------- (S, A, R, S') ----------\n",
    "\n",
    "    screen = data[step][0]               # 状态S\n",
    "    action = np.argmax(data[step][1][:5])    # 动作A\n",
    "    next_screen = data[step+1][0]        # 新状态S'\n",
    "    reward = sekiro_agent.reward_system.get_reward(get_status(next_screen))    # 奖励R\n",
    "\n",
    "    # ---------- store ----------\n",
    "\n",
    "    # 集齐 (S, A, R, S')，开始存储\n",
    "    sekiro_agent.replayer.store(\n",
    "        cv2.resize(roi(screen, x, x_w, y, y_h), (ROI_WIDTH, ROI_HEIGHT)),    # 截取感兴趣区域并图像缩放\n",
    "        action,\n",
    "        reward,\n",
    "        cv2.resize(roi(next_screen, x, x_w, y, y_h), (ROI_WIDTH, ROI_HEIGHT))    # 截取感兴趣区域并图像缩放\n",
    "    )    # 存储经验\n",
    "\n",
    "    # ---------- learn ----------\n",
    "\n",
    "    sekiro_agent.step = step + 1\n",
    "    sekiro_agent.learn()\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## learn_online.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[查看代码](https://github.com/ricagj/pysekiro_with_RL/blob/main/learn_online.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由 collect_data.py 中 Data_collection.collect_data() 变化而来"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按 'T' 开始，然后等待按 'P' 结束\n",
    "~~~python\n",
    "if save_path:    # 判断是训练模式还是测试模式\n",
    "    train = True\n",
    "else:\n",
    "    train = False\n",
    "\n",
    "paused = True\n",
    "print(\"Ready!\")\n",
    "\n",
    "step = 0    # 计步器\n",
    "\n",
    "while True:\n",
    "\n",
    "    last_time = time.time()\n",
    "    keys = key_check()\n",
    "    if paused:\n",
    "        screen = get_screen()    # 首个 状态S，但是在按 'T' 之前，它会不断更新\n",
    "        if 'T' in keys:\n",
    "            paused = False\n",
    "            print('\\nStarting!')\n",
    "    else:\n",
    "\n",
    "        step += 1\n",
    "        \n",
    "        \"\"\"\n",
    "        核心部分，看下面\n",
    "        \"\"\"\n",
    "\n",
    "        # 降低数据采集的频率，两次采集的时间间隔为0.1秒\n",
    "        t = 0.1-(time.time()-last_time)\n",
    "        if t > 0:\n",
    "            time.sleep(t)\n",
    "\n",
    "        if 'P' in keys:\n",
    "            if train:\n",
    "                sekiro_agent.save_evaluate_network()    # 学习完毕，保存网络权重\n",
    "            sekiro_agent.reward_system.save_reward_curve(save_path=reward_curve_save_path)    # 绘制 reward 曲线并保存在当前目录\n",
    "            break\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**核心部分**  \n",
    "- 按 'T' 之后，获得首个状态S，然后依次获得 动作A、奖励R 和 新状态S'，再进入第二个轮回  \n",
    "- 第二个轮回开始\n",
    "    1. 原本的 **新状态S'** 变成 **当前状态S**\n",
    "    2. 由 **当前状态S** 选取 **动作A**\n",
    "    3. 由 **当前状态S** 获取 **奖励R**\n",
    "    4. 观测 **新状态S'**\n",
    "    - 进入下一个轮回\n",
    "- 这样做的目的是\n",
    "    1. 集齐 (S, A, R, S')\n",
    "    2. 保证 状态S 和 新状态S' 连续\n",
    "\n",
    "~~~python\n",
    "# ---------- (S, A, R, S') ----------\n",
    "\n",
    "action = sekiro_agent.choose_action(screen, train)    # 动作A\n",
    "next_screen = get_screen()    # 新状态S'\n",
    "reward = sekiro_agent.reward_system.get_reward(get_status(next_screen))    # 奖励R\n",
    "\n",
    "# ---------- 下一个轮回 ----------\n",
    "\n",
    "screen = next_screen    # 状态S\n",
    "\n",
    "\"\"\"\n",
    "学习部分，看下面\n",
    "\"\"\"\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**学习部分**  \n",
    "~~~python\n",
    "if train:\n",
    "    if not (np.sum(screen == 0) > 97200):    # 270 * 480 * 3 / 4 = 97200 ，当图像有1/4变成黑色（像素值为0）的时候停止暂停存储数据\n",
    "\n",
    "        # ---------- store ----------\n",
    "\n",
    "        # 集齐 (S, A, R, S')，开始存储\n",
    "        sekiro_agent.replayer.store(\n",
    "            cv2.resize(roi(screen, x, x_w, y, y_h), (ROI_WIDTH, ROI_HEIGHT)),    # 截取感兴趣区域并图像缩放\n",
    "            action,\n",
    "            reward,\n",
    "            cv2.resize(roi(next_screen, x, x_w, y, y_h), (ROI_WIDTH, ROI_HEIGHT))    # 截取感兴趣区域并图像缩放\n",
    "        )    # 存储经验\n",
    "\n",
    "        # ---------- learn ----------\n",
    "\n",
    "        sekiro_agent.step += 1\n",
    "        sekiro_agent.learn()\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[查看代码](https://github.com/ricagj/pysekiro_with_RL/blob/main/pysekiro/Agent.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[参考来源1](https://github.com/ZhiqingXiao/rl-book/blob/master/chapter10_atari/BreakoutDeterministic-v4_tf.ipynb)  \n",
    "[参考来源2](https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/DQN3/)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 class RewardSystem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算 reward 的原理\n",
    "~~~python\n",
    "# 约束状态值的上下限，防止异常值和特殊值的影响。\n",
    "def limit(value, lm1, lm2):\n",
    "\n",
    "    if value < lm1:\n",
    "        return lm1\n",
    "    elif value > lm2:\n",
    "        return lm2\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "# 自身状态的计算方法：(下一个的状态 - 当前的状态) * 正负强化权重\n",
    "s1 = (self.next_status[0] - self.cur_status[0]) *  1    # 自身生命\n",
    "s2 = (self.next_status[1] - self.cur_status[1]) * -1    # 自身架势\n",
    "\n",
    "# 目标状态的计算方法：约束上下限(下一个的状态 - 当前的状态) * 正负强化权重\n",
    "t1 = limit((self.next_status[2] - self.cur_status[2]), -100,   0) * -1    # 目标生命\n",
    "t2 = limit((self.next_status[3] - self.cur_status[3]),  -20, +20) *  1    # 目标架势\n",
    "\n",
    "# 分开生命值和架势并赋予这样的权重是为了降低生命值状态变化的影响\n",
    "reward = 0.1 * (s1 + t1) + 0.9 * (s2 + t2)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 class DQNReplayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个不是我写的的  \n",
    "[来源](https://github.com/ZhiqingXiao/rl-book/blob/master/chapter10_atari/BreakoutDeterministic-v4_tf.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 class Sekiro_Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~python\n",
    "def __init__(\n",
    "    self,\n",
    "    n_action = n_action, \n",
    "    gamma = 0.99,\n",
    "    batch_size = 128,\n",
    "    replay_memory_size = 20000,\n",
    "    epsilon = 1.0,\n",
    "    epsilon_decrease_rate = 0.999,\n",
    "    update_freq = 100,\n",
    "    target_network_update_freq = 300,\n",
    "    model_weights = None,\n",
    "    save_path = None\n",
    "):\n",
    "    self.n_action = n_action    # 动作数量\n",
    "\n",
    "    self.gamma = gamma    # 奖励衰减\n",
    "\n",
    "    self.batch_size = batch_size                    # 样本抽取数量\n",
    "    self.replay_memory_size = replay_memory_size    # 记忆容量\n",
    "\n",
    "    self.epsilon = epsilon                                # 探索参数\n",
    "    self.epsilon_decrease_rate = epsilon_decrease_rate    # 探索衰减率\n",
    "\n",
    "    self.update_freq = update_freq    # 训练评估网络的频率\n",
    "    self.target_network_update_freq = target_network_update_freq    # 更新目标网络的频率\n",
    "\n",
    "    self.model_weights = model_weights    # 指定读取的模型参数的路径\n",
    "    self.save_path = save_path            # 指定模型权重保存的路径\n",
    "    if not self.save_path:\n",
    "        self.save_path = 'tmp_weights.h5'\n",
    "\n",
    "    self.evaluate_net = self.build_network()    # 评估网络\n",
    "    self.target_net = self.build_network()      # 目标网络\n",
    "    self.reward_system = RewardSystem()                     # 奖惩系统\n",
    "    self.replayer = DQNReplayer(self.replay_memory_size)    # 经验回放\n",
    "\n",
    "    self.step = 0    # 计步\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 评估网络和目标网络的构建方法\n",
    "~~~python\n",
    "def build_network(self):\n",
    "    model = MODEL(ROI_WIDTH, ROI_HEIGHT, FRAME_COUNT,\n",
    "        outputs = self.n_action,\n",
    "        model_weights = self.model_weights\n",
    "    )\n",
    "    return model\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 行为选择与执行方法\n",
    "~~~python\n",
    "def choose_action(self, screen, train):\n",
    "    if train:\n",
    "        r = np.random.rand()\n",
    "    else:\n",
    "        r = 1.01    # 永远大于 self.epsilon\n",
    "\n",
    "    # train = True 开启探索模式\n",
    "    if r < self.epsilon:\n",
    "        self.epsilon *= self.epsilon_decrease_rate    # 逐渐减小探索参数, 降低行为的随机性\n",
    "        action = np.random.randint(self.n_action)\n",
    "\n",
    "    # train = False 直接进入这里\n",
    "    else:\n",
    "        screen = cv2.resize(roi(screen, x, x_w, y, y_h), (ROI_WIDTH, ROI_HEIGHT)).reshape(-1, ROI_WIDTH, ROI_HEIGHT, FRAME_COUNT)\n",
    "        q_values = self.evaluate_net.predict(screen)[0]\n",
    "        action = np.argmax(q_values)\n",
    "\n",
    "    # 执行动作\n",
    "    act(action)\n",
    "\n",
    "    return action\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 学习方法\n",
    "~~~python\n",
    "def learn(self):\n",
    "\n",
    "    if self.step >= self.batch_size and self.step % self.update_freq == 0:    # 更新评估网络\n",
    "\n",
    "        if self.step % self.target_network_update_freq == 0:    # 更新目标网络\n",
    "            print(f'step:{self.step:>4}, current_cumulative_reward:{self.reward_system.current_cumulative_reward:>5.3f}, memory:{self.replayer.count:7>}')\n",
    "            self.update_target_network() \n",
    "\n",
    "        # 经验回放\n",
    "        screens, actions, rewards, next_screens = self.replayer.sample(self.batch_size)\n",
    "\n",
    "        screens = screens.reshape(-1, ROI_WIDTH, ROI_HEIGHT, FRAME_COUNT)\n",
    "        next_screens = next_screens.reshape(-1, ROI_WIDTH, ROI_HEIGHT, FRAME_COUNT)\n",
    "\n",
    "        # 计算回报的估计值\n",
    "        q_next = self.target_net.predict(next_screens)\n",
    "        q_target = self.evaluate_net.predict(screens)\n",
    "        q_target[range(self.batch_size), actions] = rewards + self.gamma * q_next.max(axis=-1)\n",
    "\n",
    "        self.evaluate_net.fit(screens, q_target, verbose=0)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 更新目标网络权重方法\n",
    "~~~python\n",
    "def update_target_network(self):\n",
    "    self.target_net.set_weights(self.evaluate_net.get_weights())\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 保存评估网络权重方法\n",
    "~~~python\n",
    "def save_evaluate_network(self):\n",
    "    self.evaluate_net.save_weights(self.save_path)\n",
    "~~~"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
