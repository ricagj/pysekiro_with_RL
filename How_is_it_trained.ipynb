{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 了解数据如何收集，数据的格式\n",
    "- 了解如何用深度学习训练\n",
    "- 了解深度强化学习中的离线学习（在数据集中学习）是如何进行的\n",
    "- 了解深度强化学习中的在线学习（直接在游戏中学习）是如何进行的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 collect_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[查看代码](https://github.com/ricagj/pysekiro_with_RL/blob/main/pysekiro/collect_data.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 get_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对按键信息进行独热编码  \n",
    "注意，这部分的定义非常重要，后面的很多设置都与它相适应"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~python\n",
    "def get_output(keys):    # 对按键信息进行独热编码\n",
    "\n",
    "    output = [0, 0, 0, 0, 0]\n",
    "\n",
    "    if   'J' in keys:\n",
    "        output[0] = 1    # 等同于[1, 0, 0, 0, 0]\n",
    "    elif 'K' in keys:\n",
    "        output[1] = 1    # 等同于[0, 1, 0, 0, 0]\n",
    "    elif 'LSHIFT' in keys:\n",
    "        output[2] = 1    # 等同于[0, 0, 1, 0, 0]\n",
    "    elif 'SPACE' in keys:\n",
    "        output[3] = 1    # 等同于[0, 0, 0, 1, 0]\n",
    "    else:\n",
    "        output[4] = 1    # 等同于[0, 0, 0, 0, 1]\n",
    "\n",
    "    return output\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 class Data_collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.1 def \\__init__(self, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~python\n",
    "self.target = target    # 目标\n",
    "self.dataset = list()    # 保存数据的容器\n",
    "self.save_path = os.path.join('The_battle_memory', self.target)    # 保存的位置\n",
    "self.reward_system = RewardSystem()    # 奖惩系统\n",
    "\n",
    "self.step = 0    # 计步器\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.2 def save_data(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保存数据，统一名称为 f'training_data-{n}.npy'，例如 training_data-123.npy  \n",
    "从1开始检测文件名是否存在于保存路径，直到检测到不存在就保存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~python\n",
    "n = 1\n",
    "while True:    # 直到找到保存位置并保存就 break\n",
    "    filename = f'training_data-{n}.npy'\n",
    "    save_path = os.path.join(self.save_path, filename)\n",
    "    if not os.path.exists(save_path):    # 没有重复的文件名就执行保存并退出\n",
    "        np.save(save_path, self.dataset)\n",
    "        break\n",
    "    n += 1\n",
    "return filename[:-4]\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.3def collect_data(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按 'T' 开始，然后等待按 'P' 结束\n",
    "~~~python\n",
    "def collect_data(self):\n",
    "\n",
    "    paused = True\n",
    "    while True:\n",
    "        last_time = time.time()\n",
    "        keys = key_check()\n",
    "        if paused:\n",
    "            if 'T' in keys:\n",
    "                paused = False\n",
    "        else:\n",
    "\n",
    "            self.step += 1\n",
    "\n",
    "            \"\"\"\n",
    "            核心部分，看下面\n",
    "            \"\"\"\n",
    "\n",
    "            if 'P' in keys:    # 结束，保存数据\n",
    "                filename = self.save_data()    # 保存数据，保存结束后返回符合条件的文件名\n",
    "                self.reward_system.save_reward_curve(\n",
    "                    save_path = os.path.join('Data_quality', self.target, filename+'.png')\n",
    "                )    # 绘制 reward 曲线并保存在当前目录\n",
    "                break\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**核心部分**  \n",
    "- 记录数据的条件是非黑屏  \n",
    "- 数据采集降频的目的是缓解数据不平衡和提高 action 与 reward 的对应程度\n",
    "    - 按键检测的原理是检测当前键盘上正在按着的键，我们的操作有按键有松键，按键对应了4种动作（攻防垫跳），松键只对应1个动作（其他），所以会有大量数据集中在松键状态，而采集频率的增加则会加剧这种情况。\n",
    "    - 游戏接受玩家的按键信号，然后让人物执行相应的动作，过一会玩家从屏幕观测到状态变化（生命值和架势的变化）。如果采集频率过高，会导致状态变化可能存在好几个新状态之后， action 与 reward 不对应会导致训练出现严重的后果甚至训练结果是无效的。\n",
    "~~~python\n",
    "screen = get_screen()    # 获取屏幕图像\n",
    "if not (np.sum(screen == 0) > 5000):    # 正常情况下不会有那么多值为0的像素点，除非黑屏了\n",
    "    action = get_output(keys)    # 获取按键输出\n",
    "    self.dataset.append([screen, action])    # 图像和输出打包在一起，保证一一对应\n",
    "\n",
    "    status = get_status(screen)\n",
    "    reward = self.reward_system.get_reward(status)    # 计算 reward\n",
    "\n",
    "    # 降低数据采集的频率，两次采集的时间间隔为0.1秒\n",
    "    t = 0.1-(time.time()-last_time)\n",
    "    if t > 0:\n",
    "        time.sleep(t)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 数据预览"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意：请先收集至少一个数据集**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data_quality (存放收集数据时记录的reward曲线)\n",
    "    - Genichiro_Ashina （苇名弦一郎）\n",
    "        - training_data-1.png （第一个战斗数据的reward曲线）\n",
    "- \n",
    "- The_battle_memory\n",
    "    - Genichiro_Ashina （苇名弦一郎）\n",
    "        - training_data-1.npy （第一个战斗数据）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前数据集所包含数据量：1666, 每个数据都有 2 个数据，分别是图像数据和按键数据\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "target = 'Genichiro_Ashina' # 苇名弦一郎\n",
    "path = os.path.join('The_battle_memory', target, f'training_data-{1}.npy')\n",
    "data = np.load(path, allow_pickle=True)\n",
    "print(f'当前数据集所包含数据量：{data.shape[0]}, 每个数据都有 {data.shape[1]} 个数据，分别是图像数据和按键数据')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "图像 [[77 85 74 ... 61 63 63]\n",
      " [60 36 42 ... 24 25 24]\n",
      " [58 53 41 ... 24 25 29]\n",
      " ...\n",
      " [33 27 28 ... 74 67 71]\n",
      " [35 31 30 ... 85 89 89]\n",
      " [38 33 32 ... 71 73 73]]\n",
      "形状 (270, 480)\n"
     ]
    }
   ],
   "source": [
    "screen = data[0][0]    # 选取了第一个数据的图像部分\n",
    "print('图像', screen)\n",
    "print('形状', screen.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "按键记录 [0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "action_value = data[0][1]    # 选取了第一个数据的按键记录部分\n",
    "print('按键记录', action_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "motion:  攻击, 记录到的次数:   486\n",
      "motion:  弹反, 记录到的次数:   130\n",
      "motion:  垫步, 记录到的次数:    40\n",
      "motion:  跳跃, 记录到的次数:     4\n",
      "motion:  其他, 记录到的次数:  1006\n"
     ]
    }
   ],
   "source": [
    "# 统计动作分布\n",
    "def motionCounts(Y):\n",
    "\n",
    "    columns=['攻击', '弹反', '垫步', '跳跃', '其他']\n",
    "    df = pd.DataFrame(Y, columns=columns, dtype=np.uint8)\n",
    "    \n",
    "    total = len(df)\n",
    "\n",
    "    for motion in columns:\n",
    "        motion_count = len(df)-df[motion].value_counts()[0]\n",
    "        print(f'motion: {motion:>3}, 记录到的次数: {motion_count:>5}')\n",
    "\n",
    "Y = np.array([i[1] for i in data])\n",
    "motionCounts(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以视频的形式展示数据\n",
      " 剩余:  149, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:  148, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:  147, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:  146, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:  145, 动作:垫步, [0, 0, 1, 0, 0]\n",
      " 剩余:  144, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:  143, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:  142, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:  141, 动作:弹反, [0, 1, 0, 0, 0]\n",
      " 剩余:  140, 动作:弹反, [0, 1, 0, 0, 0]\n",
      " 剩余:  139, 动作:弹反, [0, 1, 0, 0, 0]\n",
      " 剩余:  138, 动作:弹反, [0, 1, 0, 0, 0]\n",
      " 剩余:  137, 动作:弹反, [0, 1, 0, 0, 0]\n",
      " 剩余:  136, 动作:弹反, [0, 1, 0, 0, 0]\n",
      " 剩余:  135, 动作:弹反, [0, 1, 0, 0, 0]\n",
      " 剩余:  134, 动作:弹反, [0, 1, 0, 0, 0]\n",
      " 剩余:  133, 动作:弹反, [0, 1, 0, 0, 0]\n",
      " 剩余:  132, 动作:弹反, [0, 1, 0, 0, 0]\n",
      " 剩余:  131, 动作:弹反, [0, 1, 0, 0, 0]\n",
      " 剩余:  130, 动作:弹反, [0, 1, 0, 0, 0]\n",
      " 剩余:  129, 动作:弹反, [0, 1, 0, 0, 0]\n",
      " 剩余:  128, 动作:弹反, [0, 1, 0, 0, 0]\n",
      " 剩余:  127, 动作:弹反, [0, 1, 0, 0, 0]\n",
      " 剩余:  126, 动作:弹反, [0, 1, 0, 0, 0]\n",
      " 剩余:  125, 动作:弹反, [0, 1, 0, 0, 0]\n",
      " 剩余:  124, 动作:弹反, [0, 1, 0, 0, 0]\n",
      " 剩余:  123, 动作:弹反, [0, 1, 0, 0, 0]\n",
      " 剩余:  122, 动作:弹反, [0, 1, 0, 0, 0]\n",
      " 剩余:  121, 动作:弹反, [0, 1, 0, 0, 0]\n",
      " 剩余:  120, 动作:弹反, [0, 1, 0, 0, 0]\n",
      " 剩余:  119, 动作:弹反, [0, 1, 0, 0, 0]\n",
      " 剩余:  118, 动作:弹反, [0, 1, 0, 0, 0]\n",
      " 剩余:  117, 动作:弹反, [0, 1, 0, 0, 0]\n",
      " 剩余:  116, 动作:弹反, [0, 1, 0, 0, 0]\n",
      " 剩余:  115, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:  114, 动作:攻击, [1, 0, 0, 0, 0]\n",
      " 剩余:  113, 动作:攻击, [1, 0, 0, 0, 0]\n",
      " 剩余:  112, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:  111, 动作:攻击, [1, 0, 0, 0, 0]\n",
      " 剩余:  110, 动作:攻击, [1, 0, 0, 0, 0]\n",
      " 剩余:  109, 动作:攻击, [1, 0, 0, 0, 0]\n",
      " 剩余:  108, 动作:攻击, [1, 0, 0, 0, 0]\n",
      " 剩余:  107, 动作:攻击, [1, 0, 0, 0, 0]\n",
      " 剩余:  106, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:  105, 动作:攻击, [1, 0, 0, 0, 0]\n",
      " 剩余:  104, 动作:攻击, [1, 0, 0, 0, 0]\n",
      " 剩余:  103, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:  102, 动作:攻击, [1, 0, 0, 0, 0]\n",
      " 剩余:  101, 动作:攻击, [1, 0, 0, 0, 0]\n",
      " 剩余:  100, 动作:攻击, [1, 0, 0, 0, 0]\n",
      " 剩余:   99, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:   98, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:   97, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:   96, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:   95, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:   94, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:   93, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:   92, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:   91, 动作:攻击, [1, 0, 0, 0, 0]\n",
      " 剩余:   90, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:   89, 动作:攻击, [1, 0, 0, 0, 0]\n",
      " 剩余:   88, 动作:攻击, [1, 0, 0, 0, 0]\n",
      " 剩余:   87, 动作:攻击, [1, 0, 0, 0, 0]\n",
      " 剩余:   86, 动作:攻击, [1, 0, 0, 0, 0]\n",
      " 剩余:   85, 动作:攻击, [1, 0, 0, 0, 0]\n",
      " 剩余:   84, 动作:攻击, [1, 0, 0, 0, 0]\n",
      " 剩余:   83, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:   82, 动作:攻击, [1, 0, 0, 0, 0]\n",
      " 剩余:   81, 动作:攻击, [1, 0, 0, 0, 0]\n",
      " 剩余:   80, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:   79, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:   78, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:   77, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:   76, 动作:垫步, [0, 0, 1, 0, 0]\n",
      " 剩余:   75, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:   74, 动作:攻击, [1, 0, 0, 0, 0]\n",
      " 剩余:   73, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:   72, 动作:攻击, [1, 0, 0, 0, 0]\n",
      " 剩余:   71, 动作:攻击, [1, 0, 0, 0, 0]\n",
      " 剩余:   70, 动作:攻击, [1, 0, 0, 0, 0]\n",
      " 剩余:   69, 动作:攻击, [1, 0, 0, 0, 0]\n",
      " 剩余:   68, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:   67, 动作:攻击, [1, 0, 0, 0, 0]\n",
      " 剩余:   66, 动作:攻击, [1, 0, 0, 0, 0]\n",
      " 剩余:   65, 动作:攻击, [1, 0, 0, 0, 0]\n",
      " 剩余:   64, 动作:攻击, [1, 0, 0, 0, 0]\n",
      " 剩余:   63, 动作:攻击, [1, 0, 0, 0, 0]\n",
      " 剩余:   62, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:   61, 动作:攻击, [1, 0, 0, 0, 0]\n",
      " 剩余:   60, 动作:攻击, [1, 0, 0, 0, 0]\n",
      " 剩余:   59, 动作:攻击, [1, 0, 0, 0, 0]\n",
      " 剩余:   58, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:   57, 动作:攻击, [1, 0, 0, 0, 0]\n",
      " 剩余:   56, 动作:攻击, [1, 0, 0, 0, 0]\n",
      " 剩余:   55, 动作:攻击, [1, 0, 0, 0, 0]\n",
      " 剩余:   54, 动作:攻击, [1, 0, 0, 0, 0]\n",
      " 剩余:   53, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:   52, 动作:攻击, [1, 0, 0, 0, 0]\n",
      " 剩余:   51, 动作:攻击, [1, 0, 0, 0, 0]\n",
      " 剩余:   50, 动作:攻击, [1, 0, 0, 0, 0]\n",
      " 剩余:   49, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:   48, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:   47, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:   46, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:   45, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:   44, 动作:攻击, [1, 0, 0, 0, 0]\n",
      " 剩余:   43, 动作:攻击, [1, 0, 0, 0, 0]\n",
      " 剩余:   42, 动作:攻击, [1, 0, 0, 0, 0]\n",
      " 剩余:   41, 动作:攻击, [1, 0, 0, 0, 0]\n",
      " 剩余:   40, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:   39, 动作:攻击, [1, 0, 0, 0, 0]\n",
      " 剩余:   38, 动作:攻击, [1, 0, 0, 0, 0]\n",
      " 剩余:   37, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:   36, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:   35, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:   34, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:   33, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:   32, 动作:攻击, [1, 0, 0, 0, 0]\n",
      " 剩余:   31, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:   30, 动作:攻击, [1, 0, 0, 0, 0]\n",
      " 剩余:   29, 动作:攻击, [1, 0, 0, 0, 0]\n",
      " 剩余:   28, 动作:攻击, [1, 0, 0, 0, 0]\n",
      " 剩余:   27, 动作:攻击, [1, 0, 0, 0, 0]\n",
      " 剩余:   26, 动作:攻击, [1, 0, 0, 0, 0]\n",
      " 剩余:   25, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:   24, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:   23, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:   22, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:   21, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:   20, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:   19, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:   18, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:   17, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:   16, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:   15, 动作:垫步, [0, 0, 1, 0, 0]\n",
      " 剩余:   14, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:   13, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:   12, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:   11, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:   10, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:    9, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:    8, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:    7, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:    6, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:    5, 动作:跳跃, [0, 0, 0, 1, 0]\n",
      " 剩余:    4, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:    3, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:    2, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:    1, 动作:其他, [0, 0, 0, 0, 1]\n",
      " 剩余:    0, 动作:攻击, [1, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print('以视频的形式展示数据')\n",
    "data = data[-200:-50]    # 展示其中150个\n",
    "Remaining = len(data)\n",
    "for screen, action_value in data:\n",
    "    action = np.argmax(action_value)\n",
    "\n",
    "    if   action == 0:\n",
    "        action = '攻击'\n",
    "    elif action == 1:\n",
    "        action = '弹反'\n",
    "    elif action == 2:\n",
    "        action = '垫步'\n",
    "    elif action == 3:\n",
    "        action = '跳跃'\n",
    "    elif action == 4:\n",
    "        action = '其他'\n",
    "\n",
    "    cv2.imshow('screen', screen)\n",
    "    cv2.waitKey(30)\n",
    "\n",
    "    Remaining -= 1\n",
    "    print(f'\\r 剩余: {Remaining:>4}, 动作:{action}, {action_value}', end='\\n') # end=''\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):    # 按 q 键关闭视频\n",
    "        cv2.destroyAllWindows()\n",
    "        break\n",
    "else:\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：我提供的模型不是最优的，有能力的可以自己重写  \n",
    "要求：输入形状相关参数（width, height, frame_count），输出相关参数（outputs），需要编译（model.compile）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 train_with_dl.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[查看代码](https://github.com/ricagj/pysekiro_with_RL/blob/main/pysekiro/train_with_dl.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **target** 指定训练的对象\n",
    "- 注：必须 start < end\n",
    "    - **start** 指定第一个训练的数据集\n",
    "    - **end** 指定最后一个训练的数据集\n",
    "- **batch_size** 训练时的批大小\n",
    "- **epochs** 每个数据集训练的次数\n",
    "    - 如果你搜集的数据足够多，并且是按照上面的要求做的，那完全可以设置成1\n",
    "    - 因为既然每个数据都是一场完整的战斗，那么真正的epochs其实是数据集的数量，除非你每场战斗是风格都不一样\n",
    "- **model_weights** 用以支持增量学习。默认为None，从零开始训练。\n",
    "    - 当你搜集新的数据集之后想继续训练，可以把这个参数设置为要继续训练的模型的路径，这样就可以继续训练了\n",
    "    - 当你在训练过程中发生了某种意外终止了训练，可以把这个参数设置为要继续训练的模型的路径，然后把发生意外时正在训练的那个数据集的序号设置为start，就可以继续训练了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~python\n",
    "def train(\n",
    "    target,\n",
    "    start=1,\n",
    "    end=1,\n",
    "    batch_size=8,\n",
    "    epochs=1,\n",
    "    model_weights=None\n",
    "    ):\n",
    "    \n",
    "    model = MODEL(ROI_WIDTH, ROI_HEIGHT, FRAME_COUNT,\n",
    "        outputs = n_action,\n",
    "        model_weights = model_weights\n",
    "    )\n",
    "    model.summary()\n",
    "    \n",
    "    \"\"\"\n",
    "    核心部分，看下面\n",
    "    \"\"\"\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**核心部分**\n",
    "~~~python\n",
    "model_weights = 'dl_weights.h5'\n",
    "\n",
    "# 读取一个数据集训练，然后再读取下一个数据集训练，以此类推\n",
    "for i in range(start, end+1):\n",
    "\n",
    "    filename = f'training_data-{i}.npy'\n",
    "    data_path = os.path.join('The_battle_memory', target, filename)\n",
    "\n",
    "    if os.path.exists(data_path):    # 确保数据集存在\n",
    "        \"\"\"\n",
    "        训练部分，看下面\n",
    "        \"\"\"\n",
    "    else:\n",
    "        print(f'{filename} does not exist ')\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**训练部分**\n",
    "~~~python\n",
    "# 加载数据集\n",
    "data = np.load(data_path, allow_pickle=True)\n",
    "\n",
    "# 数据集处理成预训练格式\n",
    "X = np.array([roi(i[0], x, x_w, y, y_h) for i in data]).reshape(-1, ROI_WIDTH, ROI_HEIGHT, FRAME_COUNT)\n",
    "Y = np.array([i[1] for i in data])\n",
    "\n",
    "# 训练模型，然后保存\n",
    "model.fit(X, Y, batch_size=batch_size, epochs=epochs, verbose=1)\n",
    "model.save_weights(model_weights)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 learn_offline.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[查看代码](https://github.com/ricagj/pysekiro_with_RL/blob/main/learn_offline.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由 train_with_dl.py 升级而来"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **target** 指定训练的对象\n",
    "- 注：必须 start < end\n",
    "    - **start** 指定第一个训练的数据集\n",
    "    - **end** 指定最后一个训练的数据集\n",
    "- **model_weights** 用以支持增量学习。默认为None。没有指定需要增量学习的模型的路径的话，就从零开始训练。\n",
    "    - 当你搜集新的数据集之后想继续训练，可以把这个参数设置为要继续训练的模型的路径，这样就可以继续训练了\n",
    "    - 当你在训练过程中发生了某种意外终止了训练，可以把这个参数设置为要继续训练的模型的路径，然后把发生意外时正在训练的那个数据集的序号设置为start，就可以继续训练了\n",
    "- **save_path** 指定模型权重保存的路径"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~python\n",
    "def learn_offline(\n",
    "    target,\n",
    "    start =1,\n",
    "    end = 1,\n",
    "    model_weights=None,\n",
    "    save_path=None\n",
    "    )\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~python\n",
    "# 依次读取训练集进行离线学习\n",
    "for i in range(start, end+1):\n",
    "\n",
    "    filename = f'training_data-{i}.npy'\n",
    "    data_path = os.path.join('The_battle_memory', target, filename)\n",
    "\n",
    "    if os.path.exists(data_path):    # 确保数据集存在\n",
    "\n",
    "        # 加载数据集\n",
    "        data = np.load(data_path, allow_pickle=True)\n",
    "\n",
    "        \"\"\"\n",
    "        学习过程，看下面\n",
    "        \"\"\"\n",
    "        \n",
    "        sekiro_agent.save_evaluate_network()    # 这个数据学习完毕，保存网络权重\n",
    "        sekiro_agent.reward_system.save_reward_curve()    # 绘制 reward 曲线并保存在当前目录\n",
    "\n",
    "    else:\n",
    "        print(f'{filename} does not exist ')\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**学习过程**  \n",
    "主要思路：获取 (状态S、动作A、奖励R 和 新状态S')，存储以供经验回放，然后目标网络和评估网络根据各自设置的更新频率更新自身网络参数  \n",
    "~~~python\n",
    "for step in range(len(data)-1):\n",
    "\n",
    "    # ---------- (S, A, R, S') ----------\n",
    "    \n",
    "    # 读取 状态S、动作A 和 新状态S'\n",
    "    screen = data[step][0]           # 状态S\n",
    "    action = data[step][1]           # 动作A\n",
    "    next_screen = data[step+1][0]    # 新状态S'\n",
    "\n",
    "    # 获取 奖励R\n",
    "    status = get_status(screen)\n",
    "    reward = sekiro_agent.reward_system.get_reward(status)    # 奖励R\n",
    "\n",
    "    # ---------- store ----------\n",
    "    \n",
    "    # 集齐 (S, A, R, S')，开始存储\n",
    "    sekiro_agent.replayer.store(\n",
    "        roi(screen, x, x_w, y, y_h),\n",
    "        np.argmax(action),\n",
    "        reward,\n",
    "        roi(next_screen, x, x_w, y, y_h)\n",
    "    )    # 存储经验\n",
    "\n",
    "    # ---------- train ----------\n",
    "    \n",
    "    if step >= sekiro_agent.batch_size:\n",
    "        if step % update_freq == 0:\n",
    "            sekiro_agent.learn()    # 更新评估网络\n",
    "            sekiro_agent.save_evaluate_network()    # 保存网络权重\n",
    "        if step % target_network_update_freq == 0:    # 更新目标网络\n",
    "            sekiro_agent.update_target_network()\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 learn_online.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[查看代码](https://github.com/ricagj/pysekiro_with_RL/blob/main/learn_online.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由 collect_data.py 变化而来"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~python\n",
    "def learn_online(model_weights=None, save_path=None)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **model_weights** 指定模型权重的路径。\n",
    "- **save_path** 指定模型权重保存的路径。注：设置该参数的同时会开始训练模式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~python\n",
    "if save_path:\n",
    "    train = True\n",
    "else:\n",
    "    train = False\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按 'T' 开始，然后等待按 'P' 结束\n",
    "~~~python\n",
    "paused = True\n",
    "\n",
    "step = 0    # 计步器\n",
    "step_train = 0    # 计步器\n",
    "\n",
    "while True:\n",
    "\n",
    "    last_time = time.time()\n",
    "    keys = key_check()\n",
    "    if paused:\n",
    "        screen = get_screen()    # 首个 状态S，但是在按 'T' 之前，它会不断更新\n",
    "        if 'T' in keys:\n",
    "            paused = False\n",
    "    else:\n",
    "\n",
    "        step += 1\n",
    "        \n",
    "        \"\"\"\n",
    "        核心部分，看下面\n",
    "        \"\"\"\n",
    "\n",
    "        # 降低数据采集的频率，两次采集的时间间隔为0.1秒\n",
    "        t = 0.1-(time.time()-last_time)\n",
    "        if t > 0:\n",
    "            time.sleep(t)\n",
    "\n",
    "        if 'P' in keys:\n",
    "            if train:\n",
    "                sekiro_agent.save_evaluate_network()    # 学习完毕，保存网络权重\n",
    "            sekiro_agent.reward_system.save_reward_curve(save_path='learn_online.png')    # 绘制 reward 曲线并保存在当前目录\n",
    "            break\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**核心部分**  \n",
    "- 按 'T' 之后，获得首个状态S，然后依次获得 动作A、奖励R 和 新状态S'，再进入下一个轮回  \n",
    "- 第二个轮回开始\n",
    "    1. 原本的 **新状态S'** 变成 **当前状态S**\n",
    "    2. 由 **当前状态S** 选取 **动作A**\n",
    "    3. 由 **当前状态S** 获取 **奖励R**\n",
    "    4. 观测 **新状态S'**\n",
    "    - 进入下一个轮回\n",
    "- 这样做的目的是\n",
    "    1. 集齐 (S, A, R, S')\n",
    "    2. 保证 状态S 和 新状态S' 连续\n",
    "\n",
    "~~~python\n",
    "# ---------- (S, A, R, S') ----------\n",
    "\n",
    "# 选取动作，同时执行动作\n",
    "action = sekiro_agent.choose_action(screen, train)    # 动作A\n",
    "\n",
    "# 获取 奖励R\n",
    "status = get_status(screen)\n",
    "reward = sekiro_agent.reward_system.get_reward(status)    # 奖励R\n",
    "\n",
    "next_screen = get_screen()    # 新状态S'\n",
    "\n",
    "# ---------- 下一个轮回 ----------\n",
    "\n",
    "screen = next_screen    # 状态S\n",
    "\n",
    "\"\"\"\n",
    "训练部分，看下面\n",
    "\"\"\"\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**训练部分**  \n",
    "~~~python\n",
    "if train:\n",
    "    if not (np.sum(screen == 0) > 5000 or np.sum(next_screen == 0) > 5000):    # 正常情况下不会有那么多值为0的像素点，除非黑屏了\n",
    "        \n",
    "        # ---------- store ----------\n",
    "        \n",
    "        # 集齐 (S, A, R, S')，开始存储\n",
    "        sekiro_agent.replayer.store(\n",
    "            roi(screen, x, x_w, y, y_h),\n",
    "            np.argmax(action),\n",
    "            reward,\n",
    "            roi(next_screen, x, x_w, y, y_h)\n",
    "        )    # 存储经验\n",
    "\n",
    "        # ---------- train ----------\n",
    "        \n",
    "        step_train += 1\n",
    "        if step_train >= sekiro_agent.batch_size:\n",
    "            if step_train % update_freq == 0:\n",
    "                sekiro_agent.learn()    # 更新评估网络\n",
    "                sekiro_agent.save_evaluate_network()    # 保存网络权重\n",
    "            if step_train % target_network_update_freq == 0:    # 更新目标网络\n",
    "                sekiro_agent.update_target_network()\n",
    "~~~"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
