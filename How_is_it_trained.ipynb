{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 了解数据如何收集，数据的格式\n",
    "- 了解如何用深度学习训练\n",
    "- 了解深度强化学习中的离线学习（在数据集中学习）是如何进行的\n",
    "- 了解深度强化学习中的在线学习（直接在游戏中学习）是如何进行的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 至于理论部分，有空再补充"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 collect_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[查看代码](https://github.com/ricagj/pysekiro_with_RL/blob/main/pysekiro/collect_data.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 get_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对按键信息进行独热编码  \n",
    "**注意，这部分的定义非常重要，后面有部分重要的代码需要根据这里的定义来编写**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更新：记录位移信息，允许使用道具（指伤药葫芦等道具）  \n",
    "但是，这些信息暂时不参与训练，以后再用。位移和道具目前被统一归到其他类。  \n",
    "参与训练的仍然是 攻击、防御、垫步、跳跃、其他类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~python\n",
    "def get_output(keys):    # 对按键信息进行独热编码\n",
    "\n",
    "    output = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "    # 攻击、防御、垫步、跳跃和使用道具不能同时进行（指0.1秒内），但是可以和移动同时进行\n",
    "    if   'J' in keys:\n",
    "        output[0] = 1    # 等同于[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    elif 'K' in keys:\n",
    "        output[1] = 1    # 等同于[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    elif 'LSHIFT' in keys:\n",
    "        output[2] = 1    # 等同于[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "    elif 'SPACE' in keys:\n",
    "        output[3] = 1    # 等同于[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "    elif 'R' in keys:\n",
    "        output[5] = 1    # 等同于[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "    else:\n",
    "        output[4] = 1    # 等同于[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "\n",
    "    # 不能同时前后移动\n",
    "    if   'W' in keys:\n",
    "        output[6] = 1    # 等同于[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
    "    elif 'S' in keys:\n",
    "        output[7] = 1    # 等同于[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "\n",
    "    # 不能同时左右移动\n",
    "    if   'A' in keys:\n",
    "        output[8] = 1    # 等同于[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
    "    elif 'D' in keys:\n",
    "        output[9] = 1    # 等同于[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "\n",
    "    return output\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 class Data_collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~python\n",
    "def __init__(self, target):\n",
    "    self.target = target    # 目标\n",
    "    self.dataset = list()    # 保存数据的容器\n",
    "    self.save_path = os.path.join('The_battle_memory', self.target)    # 保存的位置\n",
    "    if not os.path.exists(self.save_path):    # 确保保存的位置存在\n",
    "        os.mkdir(self.save_path)\n",
    "\n",
    "    self.step = 0    # 计步器\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.1 保存数据方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保存数据，统一名称为 f'training_data-{n}.npy'，例如 training_data-123.npy  \n",
    "从1开始检测文件名是否存在于保存路径，直到检测到不存在就保存，主要是为了防止文件名重复不小心把原本的文件覆盖了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~python\n",
    "def save_data(self):\n",
    "    n = 1\n",
    "    while True:    # 直到找到保存位置并保存就 break\n",
    "        filename = f'training_data-{n}.npy'\n",
    "        save_path = os.path.join(self.save_path, filename)\n",
    "        if not os.path.exists(save_path):    # 没有重复的文件名就执行保存并退出\n",
    "            print(save_path)\n",
    "            np.save(save_path, self.dataset)\n",
    "            break\n",
    "        n += 1\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.2 收集数据方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按 'T' 开始，然后等待按 'P' 结束\n",
    "~~~python\n",
    "def collect_data(self):\n",
    "\n",
    "    paused = True\n",
    "    while True:\n",
    "        last_time = time.time()\n",
    "        keys = key_check()\n",
    "        if paused:\n",
    "            if 'T' in keys:\n",
    "                paused = False\n",
    "        else:\n",
    "\n",
    "            self.step += 1\n",
    "\n",
    "            \"\"\"\n",
    "            核心部分，看下面\n",
    "            \"\"\"\n",
    "\n",
    "            if 'P' in keys:    # 结束，保存数据\n",
    "                self.save_data()    # 保存数据\n",
    "                break\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**核心部分**  \n",
    "- 记录数据的条件是非黑屏  \n",
    "- 数据采集降频的目的是缓解数据不平衡和提高 action 与 reward 的对应程度\n",
    "    - 按键检测的原理是检测当前键盘上正在按着的键，我们的操作有按键和松键，按键对应了多种动作，松键只对应1个动作（其他），所以会有大量数据集中在松键状态，而采集频率的增加则会加剧这种情况。\n",
    "    - 游戏接受玩家的按键信号，然后让人物执行相应的动作，过一会玩家从屏幕观测到状态变化（生命值和架势的变化）。如果采集频率过高，会导致状态变化可能存在于好几个新状态之后， action 与 reward 不对应会导致训练出现严重的后果甚至训练结果是无效的。\n",
    "\n",
    "~~~python\n",
    "screen = get_screen()    # 获取屏幕图像\n",
    "if not (np.sum(screen == 0) > 97200):    # 270 * 480 * 3 / 4 = 97200 ，当图像有1/4变成黑色（像素值为0）的时候停止暂停收集数据\n",
    "    action_onehot = get_output(keys)    # 获取按键输出\n",
    "    self.dataset.append([screen, action_onehot])    # 图像和输出打包在一起，保证一一对应\n",
    "\n",
    "# 降低数据采集的频率，两次采集的时间间隔为0.1秒\n",
    "t = 0.1-(time.time()-last_time)\n",
    "if t > 0:\n",
    "    time.sleep(t)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 数据预览"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注：以下展示的是我本地的数据集，我没有上传，所以要运行以下代码，请先自己收集至少一个数据集**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data_quality (存放数据集对应的reward曲线)\n",
    "    - Genichiro_Ashina （苇名弦一郎）\n",
    "        - training_data-1.png （第一个战斗数据的reward曲线）\n",
    "-\n",
    "- The_battle_memory\n",
    "    - Genichiro_Ashina （苇名弦一郎）\n",
    "        - training_data-1.npy （第一个战斗数据）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "target = 'Genichiro_Ashina' # 苇名弦一郎\n",
    "# target = 'Inner_Genichiro' # 心中的弦一郎\n",
    "# target = 'Isshin,_the_Sword_Saint' # 剑圣一心\n",
    "# target = 'Inner_Isshin' # 心中的一心\n",
    "path = os.path.join('The_battle_memory', target, f'training_data-{1}.npy')\n",
    "data = np.load(path, allow_pickle=True)\n",
    "\n",
    "print(f'当前数据集所包含数据量：{data.shape[0]}, 每个数据都有 {data.shape[1]} 个数据，分别是图像数据和按键数据')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "screen = data[0][0]    # 选取了第一个数据的图像部分\n",
    "print('图像', screen)\n",
    "print('形状', screen.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_value = data[0][1]    # 选取了第一个数据的按键记录部分\n",
    "print('按键记录', action_value)\n",
    "print('目前的训练只使用前五个  ', action_value[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计动作分布\n",
    "def motionCounts(Y):\n",
    "\n",
    "    columns=['攻击', '弹反', '垫步', '跳跃', '无键', '道具', '向前', '向后', '向左', '向右']\n",
    "    df = pd.DataFrame(Y, columns=columns, dtype=np.uint8)\n",
    "    \n",
    "    total = len(df)\n",
    "\n",
    "    for motion in columns:\n",
    "        motion_count = len(df)-df[motion].value_counts()[0]\n",
    "        print(f'motion: {motion:>3}, 记录到的次数: {motion_count:>5}')\n",
    "\n",
    "Y = np.array([i[1] for i in data])\n",
    "motionCounts(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysekiro.get_vertices import roi\n",
    "\n",
    "print()\n",
    "print('以视频的形式展示数据。', '要你自己运行才看得见视频')\n",
    "Remaining = len(data)\n",
    "for screen, action_value in data:\n",
    "    action = np.argmax(action_value)\n",
    "\n",
    "    if   action == 0:\n",
    "        a = '攻击'\n",
    "    elif action == 1:\n",
    "        a = '弹反'\n",
    "    elif action == 2:\n",
    "        a = '垫步'\n",
    "    elif action == 3:\n",
    "        a = '跳跃'\n",
    "    elif action == 4:\n",
    "        a = '其他'\n",
    "\n",
    "    cv2.imshow('screen', screen)\n",
    "    cv2.imshow('roi', roi(screen, x=140, x_w=340, y=30, y_h=230))\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "    Remaining -= 1\n",
    "    print(f'\\r 剩余: {Remaining:>4}, 动作:{a}{action_value}', end='') # end='\\n'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):    # 按 q 键关闭视频\n",
    "        cv2.destroyAllWindows()\n",
    "        break\n",
    "else:\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：我提供的模型不是最优的（*瞎搞的*），有能力的可以自己重写  \n",
    "要求：输入形状相关参数（width, height, frame_count），输出相关参数（outputs），需要编译（model.compile）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 train_with_dl.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[查看代码](https://github.com/ricagj/pysekiro_with_RL/blob/main/pysekiro/train_with_dl.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **target** 指定训练的对象\n",
    "- 注：必须 start < end\n",
    "    - **start** 指定第一个训练的数据集\n",
    "    - **end** 指定最后一个训练的数据集\n",
    "- **batch_size** 训练时的批大小\n",
    "- **epochs** 每个数据集训练的次数\n",
    "    - 如果你搜集的数据足够多，并且是按照上面的要求做的，那完全可以设置成1\n",
    "    - 因为既然每个数据都是一场完整的战斗，那么真正的epochs其实是数据集的数量，除非你每场战斗是风格都不一样\n",
    "- **model_weights** 用以支持增量学习。默认为None，从零开始训练。\n",
    "    - 当你搜集新的数据集之后想继续训练，可以把这个参数设置为要继续训练的模型的路径，这样就可以继续训练了\n",
    "    - 当你在训练过程中发生了某种意外终止了训练，可以把这个参数设置为要继续训练的模型的路径，然后把发生意外时正在训练的那个数据集的序号设置为start，就可以继续训练了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~python\n",
    "def train(\n",
    "    target,\n",
    "    start=1,\n",
    "    end=1,\n",
    "    batch_size=128,\n",
    "    epochs=1,\n",
    "    model_weights=None\n",
    "    ):\n",
    "    \n",
    "    model = MODEL(ROI_WIDTH, ROI_HEIGHT, FRAME_COUNT,\n",
    "        outputs = n_action,\n",
    "        model_weights = model_weights\n",
    "    )\n",
    "    model.summary()\n",
    "    \n",
    "    \"\"\"\n",
    "    核心部分，看下面\n",
    "    \"\"\"\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**核心部分**\n",
    "~~~python\n",
    "model_weights = 'dl_weights.h5'\n",
    "\n",
    "# 读取一个数据集训练，然后再读取下一个数据集训练，以此类推\n",
    "for i in range(start, end+1):\n",
    "\n",
    "    filename = f'training_data-{i}.npy'\n",
    "    data_path = os.path.join('The_battle_memory', target, filename)\n",
    "\n",
    "    if os.path.exists(data_path):    # 确保数据集存在\n",
    "        \n",
    "        # 加载数据集\n",
    "        data = np.load(data_path, allow_pickle=True)\n",
    "        \n",
    "        \"\"\"\n",
    "        训练部分，看下面\n",
    "        \"\"\"\n",
    "        \n",
    "    else:\n",
    "        print(f'{filename} does not exist ')\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**训练部分**\n",
    "~~~python\n",
    "# 数据集处理成预训练格式\n",
    "X = np.array([cv2.resize(roi(i[0], x, x_w, y, y_h), (ROI_WIDTH, ROI_HEIGHT)) for i in data]).reshape(-1, ROI_WIDTH, ROI_HEIGHT, FRAME_COUNT)\n",
    "Y = np.array([i[1][:5] for i in data])\n",
    "\n",
    "# 训练模型，然后保存\n",
    "model.fit(X, Y, batch_size=batch_size, epochs=epochs, verbose=1)\n",
    "model.save_weights(model_weights)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 learn_offline.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[查看代码](https://github.com/ricagj/pysekiro_with_RL/blob/main/learn_offline.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由 train_with_dl.py 升级而来"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **target** 指定训练的对象\n",
    "- 注：必须 start < end\n",
    "    - **start** 指定第一个训练的数据集\n",
    "    - **end** 指定最后一个训练的数据集\n",
    "- **model_weights** 用以支持增量学习。默认为None。没有指定需要增量学习的模型的路径的话，就从零开始训练。\n",
    "    - 当你搜集新的数据集之后想继续训练，可以把这个参数设置为要继续训练的模型的路径，这样就可以继续训练了\n",
    "    - 当你在训练过程中发生了某种意外终止了训练，可以把这个参数设置为要继续训练的模型的路径，然后把发生意外时正在训练的那个数据集的序号设置为start，就可以继续训练了\n",
    "- **save_path** 指定模型权重保存的路径"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~python\n",
    "def learn_offline(\n",
    "    target,\n",
    "    start=1,\n",
    "    end=1,\n",
    "    model_weights=None,\n",
    "    save_path=None\n",
    "    )\n",
    "\n",
    "    sekiro_agent = Sekiro_Agent(\n",
    "        model_weights = model_weights,\n",
    "        save_path = save_path\n",
    "    )\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~python\n",
    "# 依次读取训练集进行离线学习\n",
    "for i in range(start, end+1):\n",
    "\n",
    "    filename = f'training_data-{i}.npy'\n",
    "    data_path = os.path.join('The_battle_memory', target, filename)\n",
    "\n",
    "    if os.path.exists(data_path):    # 确保数据集存在\n",
    "\n",
    "        # 加载数据集\n",
    "        data = np.load(data_path, allow_pickle=True)\n",
    "\n",
    "        \"\"\"\n",
    "        学习过程，看下面\n",
    "        \"\"\"\n",
    "        \n",
    "        sekiro_agent.save_evaluate_network()    # 这个数据学习完毕，保存网络权重\n",
    "        sekiro_agent.reward_system.save_reward_curve(\n",
    "            save_path = os.path.join('Data_quality', target, filename[:-4]+'.png')\n",
    "        )    # 绘制 reward 曲线并保存\n",
    "\n",
    "    else:\n",
    "        print(f'{filename} does not exist ')\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**学习过程**  \n",
    "主要思路：获取 (状态S、动作A、奖励R 和 新状态S')，存储以供经验回放，然后目标网络和评估网络根据各自设置的更新频率更新自身网络参数  \n",
    "~~~python\n",
    "for step in range(len(data)-1):\n",
    "\n",
    "    # ---------- (S, A, R, S') ----------\n",
    "\n",
    "    screen = data[step][0]               # 状态S\n",
    "    action = np.argmax(data[step][1])    # 动作A\n",
    "    reward = sekiro_agent.reward_system.get_reward(get_status(screen))    # 奖励R\n",
    "    next_screen = data[step+1][0]        # 新状态S'\n",
    "\n",
    "    # ---------- store ----------\n",
    "\n",
    "    # 集齐 (S, A, R, S')，开始存储\n",
    "    sekiro_agent.replayer.store(\n",
    "        cv2.resize(roi(screen, x, x_w, y, y_h), (ROI_WIDTH, ROI_HEIGHT)),    # 截取感兴趣区域并图像缩放\n",
    "        action,\n",
    "        reward,\n",
    "        cv2.resize(roi(next_screen, x, x_w, y, y_h), (ROI_WIDTH, ROI_HEIGHT))    # 截取感兴趣区域并图像缩放\n",
    "    )    # 存储经验\n",
    "\n",
    "    # ---------- learn ----------\n",
    "\n",
    "    sekiro_agent.step = step\n",
    "    sekiro_agent.learn()\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 learn_online.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[查看代码](https://github.com/ricagj/pysekiro_with_RL/blob/main/learn_online.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由 collect_data.py 变化而来"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **model_weights** 指定模型权重的路径。\n",
    "- **save_path** 指定模型权重保存的路径。注：设置该参数的同时会开始训练模式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~python\n",
    "def learn_online(model_weights=None, save_path=None):\n",
    "    \n",
    "    sekiro_agent = Sekiro_Agent(\n",
    "        model_weights=model_weights,\n",
    "        save_path = save_path\n",
    "    )\n",
    "    \n",
    "    if save_path:\n",
    "        train = True\n",
    "    else:\n",
    "        train = False\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按 'T' 开始，然后等待按 'P' 结束\n",
    "~~~python\n",
    "paused = True\n",
    "\n",
    "step = 0    # 计步器\n",
    "\n",
    "while True:\n",
    "\n",
    "    last_time = time.time()\n",
    "    keys = key_check()\n",
    "    if paused:\n",
    "        screen = get_screen()    # 首个 状态S，但是在按 'T' 之前，它会不断更新\n",
    "        if 'T' in keys:\n",
    "            paused = False\n",
    "    else:\n",
    "\n",
    "        step += 1\n",
    "        \n",
    "        \"\"\"\n",
    "        核心部分，看下面\n",
    "        \"\"\"\n",
    "\n",
    "        # 降低数据采集的频率，两次采集的时间间隔为0.1秒\n",
    "        t = 0.1-(time.time()-last_time)\n",
    "        if t > 0:\n",
    "            time.sleep(t)\n",
    "\n",
    "        if 'P' in keys:\n",
    "            if train:\n",
    "                sekiro_agent.save_evaluate_network()    # 学习完毕，保存网络权重\n",
    "            sekiro_agent.reward_system.save_reward_curve(save_path='learn_online.png')    # 绘制 reward 曲线并保存在当前目录\n",
    "            break\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**核心部分**  \n",
    "- 按 'T' 之后，获得首个状态S，然后依次获得 动作A、奖励R 和 新状态S'，再进入第二个轮回  \n",
    "- 第二个轮回开始\n",
    "    1. 原本的 **新状态S'** 变成 **当前状态S**\n",
    "    2. 由 **当前状态S** 选取 **动作A**\n",
    "    3. 由 **当前状态S** 获取 **奖励R**\n",
    "    4. 观测 **新状态S'**\n",
    "    - 进入下一个轮回\n",
    "- 这样做的目的是\n",
    "    1. 集齐 (S, A, R, S')\n",
    "    2. 保证 状态S 和 新状态S' 连续\n",
    "\n",
    "~~~python\n",
    "# ---------- (S, A, R, S') ----------\n",
    "\n",
    "action = sekiro_agent.choose_action(screen, train)    # 动作A\n",
    "reward = sekiro_agent.reward_system.get_reward(get_status(screen, show=True))    # 奖励R\n",
    "next_screen = get_screen()    # 新状态S'\n",
    "\n",
    "# ---------- 下一个轮回 ----------\n",
    "\n",
    "screen = next_screen    # 状态S\n",
    "\n",
    "\"\"\"\n",
    "训练部分，看下面\n",
    "\"\"\"\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**训练部分**  \n",
    "~~~python\n",
    "if train:\n",
    "    if not (np.sum(screen == 0) > 97200):    # 270 * 480 * 3 / 4 = 97200 ，当图像有1/4变成黑色（像素值为0）的时候停止暂停存储数据\n",
    "\n",
    "        # ---------- store ----------\n",
    "\n",
    "        # 集齐 (S, A, R, S')，开始存储\n",
    "        sekiro_agent.replayer.store(\n",
    "            cv2.resize(roi(screen, x, x_w, y, y_h), (ROI_WIDTH, ROI_HEIGHT)),    # 截取感兴趣区域并图像缩放\n",
    "            action,\n",
    "            reward,\n",
    "            cv2.resize(roi(next_screen, x, x_w, y, y_h), (ROI_WIDTH, ROI_HEIGHT))    # 截取感兴趣区域并图像缩放\n",
    "        )    # 存储经验\n",
    "\n",
    "        # ---------- learn ----------\n",
    "\n",
    "        sekiro_agent.step += 1\n",
    "        sekiro_agent.learn()\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Agent.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[查看代码](https://github.com/ricagj/pysekiro_with_RL/blob/main/pysekiro/Agent.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[参考来源1](https://github.com/ZhiqingXiao/rl-book/blob/master/chapter10_atari/BreakoutDeterministic-v4_tf.ipynb)  \n",
    "[参考来源2](https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/DQN3/)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 class RewardSystem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算 reward 的原理\n",
    "~~~python\n",
    "# 约束状态值的上下限，防止异常值和特殊值的影响。\n",
    "def limit(value, lm1, lm2):\n",
    "\n",
    "    if value < lm1:\n",
    "        return lm1\n",
    "    elif value > lm2:\n",
    "        return lm2\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "# 每个状态的计算方法：(现在的状态 - 过去的状态) * 正负强化权重，然后约束上下限\n",
    "s1 = limit((self.status[0] - self.past_status[0]) *  1, -152, +76)    # 自身生命\n",
    "s2 = limit((self.status[1] - self.past_status[1]) * -1, -10, +10)    # 自身架势\n",
    "t1 = limit((self.status[2] - self.past_status[2]) * -1, -20,   0)    # 目标生命\n",
    "t2 = limit((self.status[3] - self.past_status[3]) *  1, -10, +10)    # 目标架势\n",
    "\n",
    "reward = 0.2 * (s1 + t1) + 0.8 * (s2 + t2)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 class DQNReplayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个不是我写的的  \n",
    "[来源](https://github.com/ZhiqingXiao/rl-book/blob/master/chapter10_atari/BreakoutDeterministic-v4_tf.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 class Sekiro_Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~python\n",
    "def __init__(\n",
    "    self,\n",
    "    n_action = n_action, \n",
    "    gamma = 0.99,\n",
    "    batch_size = 128,\n",
    "    replay_memory_size = 20000,\n",
    "    epsilon = 1.0,\n",
    "    epsilon_decrease_rate = 0.999,\n",
    "    update_freq = 100,\n",
    "    target_network_update_freq = 300,\n",
    "    model_weights = None,\n",
    "    save_path = None\n",
    "):\n",
    "    self.n_action = n_action    # 动作数量\n",
    "\n",
    "    self.gamma = gamma    # 奖励衰减\n",
    "\n",
    "    self.batch_size = batch_size                    # 样本抽取数量\n",
    "    self.replay_memory_size = replay_memory_size    # 记忆容量\n",
    "\n",
    "    self.epsilon = epsilon                                # 探索参数\n",
    "    self.epsilon_decrease_rate = epsilon_decrease_rate    # 探索衰减率\n",
    "\n",
    "    self.update_freq = update_freq    # 训练评估网络的频率\n",
    "    self.target_network_update_freq = target_network_update_freq    # 更新目标网络的频率\n",
    "\n",
    "    self.model_weights = model_weights    # 指定读取的模型参数的路径\n",
    "    self.save_path = save_path            # 指定模型权重保存的路径\n",
    "    if not self.save_path:\n",
    "        self.save_path = 'tmp_weights.h5'\n",
    "\n",
    "    self.evaluate_net = self.build_network()    # 评估网络\n",
    "    self.target_net = self.build_network()      # 目标网络\n",
    "    self.reward_system = RewardSystem()                     # 奖惩系统\n",
    "    self.replayer = DQNReplayer(self.replay_memory_size)    # 经验回放\n",
    "\n",
    "    self.step = 0    # 计步\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.1 评估网络和目标网络的构建方法\n",
    "~~~python\n",
    "def build_network(self):\n",
    "    model = MODEL(ROI_WIDTH, ROI_HEIGHT, FRAME_COUNT,\n",
    "        outputs = self.n_action,\n",
    "        model_weights = self.model_weights\n",
    "    )\n",
    "    return model\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.2 行为选择与执行方法\n",
    "~~~python\n",
    "def choose_action(self, screen, train):\n",
    "    if train:\n",
    "        r = np.random.rand()\n",
    "    else:\n",
    "        r = 1.01    # 永远大于 self.epsilon\n",
    "\n",
    "    # train = True 开启探索模式\n",
    "    if r < self.epsilon:\n",
    "        self.epsilon *= self.epsilon_decrease_rate    # 逐渐减小探索参数, 降低行为的随机性\n",
    "        action = np.random.randint(self.n_action)\n",
    "\n",
    "    # train = False 直接进入这里\n",
    "    else:\n",
    "        screen = roi(screen, x, x_w, y, y_h)\n",
    "        q_values = self.evaluate_net.predict([screen.reshape(-1, ROI_WIDTH, ROI_HEIGHT, FRAME_COUNT)])[0]\n",
    "        action = np.argmax(q_values)\n",
    "\n",
    "    # 执行动作\n",
    "    act(action)\n",
    "\n",
    "    return action\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.3 学习方法\n",
    "~~~python\n",
    "def learn(self):\n",
    "\n",
    "    if self.step >= self.batch_size and self.step % self.update_freq == 0:    # 更新评估网络\n",
    "\n",
    "        if self.step % self.target_network_update_freq == 0:    # 更新目标网络\n",
    "            self.update_target_network() \n",
    "\n",
    "        # 经验回放\n",
    "        screens, actions, rewards, next_screens = self.replayer.sample(self.batch_size)\n",
    "\n",
    "        screens = screens.reshape(-1, ROI_WIDTH, ROI_HEIGHT, FRAME_COUNT)\n",
    "        next_screens = next_screens.reshape(-1, ROI_WIDTH, ROI_HEIGHT, FRAME_COUNT)\n",
    "\n",
    "        # 计算回报的估计值\n",
    "        q_next = self.target_net.predict(next_screens)\n",
    "        q_target = self.evaluate_net.predict(screens)\n",
    "        q_target[range(self.batch_size), actions] = rewards + self.gamma * q_next.max(axis=-1)\n",
    "\n",
    "        self.evaluate_net.fit(screens, q_target, verbose=0)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.4 更新目标网络权重方法\n",
    "~~~python\n",
    "def update_target_network(self):\n",
    "    self.target_net.set_weights(self.evaluate_net.get_weights())\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.5 保存评估网络权重方法\n",
    "~~~python\n",
    "def save_evaluate_network(self):\n",
    "    self.evaluate_net.save_weights(self.save_path)\n",
    "~~~"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
